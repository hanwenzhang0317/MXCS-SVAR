---
title: "Fiscal Policy Shocks, Aggregate Economy and Stock Prices: Evidence from the Australian Economy"
author: "Hanwen Zhang"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This is a reserarch proposal, the project is about measuring fiscal policy effects using Bayesian Structural Vector Autoregression (SVAR) in Australian Economy. Impulse responses of stock prices and macroeconomic aggregates will be investigated. 
>
> **Keywords.** fiscal policy shock, SVAR, tax shocks, stock price, impulse response function

# Motivation 

This research project will investigate the effects on fiscal policy. How does tax cuts affect the economy? Will tax reductions boost stock market? These questions are crucial to the understanding of dynamic economics and the stimulus policy-making process, and are aimed to be answered in the background of Australian Economy in this project.  

Effects of tax shocks on output has been investigated heavily in the previous literature. For example, Mountford and Uhlig (2009) found an unanticipated deficit-financed tax cut could stimulate the economy as a fiscal policy, using sign restrictions. They also revealed that investment falls when tax increases and government spending increases as investigated by Blanchard and Perotti (2002). Romer and Romer (2007) suggested exogeneous tax changes could end with large difference. 

While fiscal policy shocks have evidence of playing a role in affecting output, the effects on stock prices is ambiguous. Afonso and Sousa (2011) illustrated government spending shocks have negative effect on stock prices, whereas expansionary tax shocks are related to increase of stock prices. However, Charziantoniou et al. (2013) argued there was no monetary policy indicators in Afonso and Sousa (2011), these authors suggested over 1991 to 2010, real and financial variables were not affected by government spending shocks significantly. 

Although there is an unignorable amount of literature regarding the effects of fiscal policy, among which a large proportion of data analysed is based on US economy. This paper aims to fill the lack of such literature about Australian economy.  

# Data 

```{r collecting data of interest}
#| echo: false
#| message: false
#| warning: false

# quarterly, seasonally adjusted, nominal, GDP per capita, unit: $
# 1973-09-01 to 2022-12-01 --> 198 quarters 
gdp_no_pc <- readabs::read_abs(series_id = "A2304420C") 
tmp.gdp_no_pc <- na.omit(xts::xts(gdp_no_pc$value, gdp_no_pc$date) )

# quarterly, seasonally adjusted, nominal, GDP, unit: $M
# 1959-09-01 to 2022-12-01 --> 254 quarters 
gdp_no <- readabs::read_abs(series_id = "A2304418T") 
tmp.gdp_no <- xts::xts(gdp_no$value, gdp_no$date) 

# Get population unit: M
pop <-  tmp.gdp_no/tmp.gdp_no_pc

# quarterly, seasonally adjusted, GDP deflator, unit: 
# 1959-09-01 to 2022-12-01 --> 254 quarters 
gdp_deflator <- readabs::read_abs(series_id = "A2303730T") 
tmp.gdp.deflator <- xts::xts(gdp_deflator$value, gdp_deflator$date) 

# construct log real GDP per capita
gdp_real_pc <- na.omit(tmp.gdp_no_pc/tmp.gdp.deflator) # 1973-09-01 to 2022-12-01

# quarterly, seasonally adjusted, government spending, unit: $M
# 1959-09-01 to 2022-12-01 --> 254 quarters 
gov_spend <- readabs::read_abs(series_id = "A2304036K") 
tmp.gov_spend  <- xts::xts(gov_spend$value, gov_spend$date) 
gov_spend_pc <- tmp.gov_spend/pop/tmp.gdp.deflator
  
# quarterly, original, total factor income, unit: $M
# 1959-09-01 to 2022-12-01 --> 254 quarters 
gov_revenue <- readabs::read_abs(series_id = "A2302411R") 
tmp.gov_revenue <- xts::xts(gov_revenue$value, gov_revenue$date) 
gov_revenue_pc <- tmp.gov_revenue / pop / tmp.gdp.deflator

# original CPI
# 1948-09-01 to 2022-12-01 --> 298 quarters 
cpi <-  readabs::read_abs(series_id = "A2325846C") 
tmp.cpi <- xts::xts(cpi$value, cpi$date) 
inflation <- 100*diff(tmp.cpi)/tmp.cpi

# Cash Rate Target; monthly average
# 1990-01-23 to 2023-03-31 
cash_r.d <- readrba::read_rba(series_id = "FIRMMCRTD") 
tmp.cash_r.d <- xts::xts(cash_r.d$value, cash_r.d$date) 
# 1990 Q1 to 2022 Q4
interest <- xts::to.quarterly(tmp.cash_r.d,OHLC=FALSE)[1:132]

# quarterly, original, National general government ;  Total liabilities ;  Total (Counterparty sectors), unit: $M 
debt <- readabs::read_abs(series_id = "A3424499W") 
tmp.debt <- xts::xts(debt$value, debt$date) 
debt.pc <- tmp.debt / pop / tmp.gdp.deflator

# ------------------------------------------------------------------------------
# Get All Ordinaries Price - daily from 1990 to 2022 
AORD.d <- read.csv("https://query1.finance.yahoo.com/v7/finance/download/AAPL?period1=631152000&period2=1672444800&interval=1d&events=history&includeAdjustedClose=true")
AORD.d$Date <- as.Date(as.character(AORD.d$Date),format="%Y-%m-%d")
tmp.aord <- xts::xts(AORD.d$Close, AORD.d$Date) 
aord_q <- xts::to.quarterly(tmp.aord,OHLC=FALSE)
aord <- as.numeric(aord_q) / tmp.gdp.deflator[123:length(tmp.gdp.deflator)]
# ------------------------------------------------------------------------------

```

The variables that are of interest include GDP, total government expenditure, total government revenue, cash rate target, inflation, public debt and stock prices. Those economic variables were collected from Reserve Bank of Australia (RBA) and Australian Bureau of Statistics (ABS), spanning quarterly from 1990 Q1 to 2022 Q4, including 132 observations. 

Stock prices are data of All Ordinaries, which is the benchmark of Australian market with relative long history, separate financial sectors might be studied into detail in the future research. Inflation and cash rate target are also introduced in the model as controls. Original data are plotted in: @fig-plot.

```{r in normial terms}
# all in real per capita terms except inflation and interest 
df.o <- as.data.frame(
  merge(tmp.gdp_no, # Nominal GDP $M
            tmp.gov_spend, # Government Spending $M
            tmp.gov_revenue, # Government Revenue $M
            tmp.debt, # Pubic Debt $M
            inflation)) # Inflation (%)
df.o <- na.omit(df.o)[8:139,] # truncate data from 1990 Q1 to 2022 Q4 
colnames(df.o) <- c("GDP","Spending","Revenue","Debt","Inflation")
df.o$Interest <- as.numeric(as.character(interest))
df.o$Stock <- as.numeric(aord_q)
```


```{r data visualization}
#| echo: false
#| message: false
#| warning: false
#| label: fig-plot
#| fig-cap: "Time Series Plots (in nominal prices)"

dates <- as.Date(rownames(df.o),format = "%Y-%m-%d")
names <- c("Nominal GDP $M","Government Spending $M","Government Revenue $M","Pubic Debt $M","Inflation (%)","Cash Rate Target (%)","Stock Price $")
col <- c("#009999","#009999", "#009999", "#009999", "#9933CC", "#9933CC", "maroon")
par(mfrow=c(4,2), mar=c(2,2,2,2))
for (i in 1:ncol(df.o)){
  plot(dates, y = df.o[,i], type = "l", 
       main = paste(names[i]), ylab = "", xlab = "",
       col = col[i], lwd = 2.5,
       ylim = c(min(na.omit(df.o[,i])),max(na.omit(df.o[,i]))))
}

```
While GDP and government expenditure depict upward trend, government revenue shows increasing trend with seasonal patterns. Public debt kept relatively flat before the Global Financial Crisis (GFC), and started to increase after. Interest rate has a downward trend while stock prices exhibiting significant drop during GFC.

Variables are transformed into real per-capita terms and compute logarithms as in Mumtaz and Theodoridis (2020), except inflation and cash rate target. In this stage, only the sample including all variables ranging from 2001 Q2 to 2022 Q4 is considered in the following section. Plots are provided as @fig-line-plot.

```{r wrangling data}
#| echo: false
#| message: false
#| warning: false

# all in real per capita terms except inflation and interest 
df.log <- as.data.frame(merge(log(gdp_real_pc),
            log(gov_spend_pc),
            log(gov_revenue_pc),
            log(debt.pc),
            inflation))
df.log <- na.omit(df.log)[8:139,] # truncate data from 1990 Q1 to 2022 Q4 
colnames(df.log) <- c("GDP","Spending","Revenue","Debt","Inflation")
df.log$Interest <- as.numeric(as.character(interest))
df.log$Stock <- log(as.numeric(aord))
```

@tbl-irf provides statistics summary of all variables from 2001Q2 to 2022Q4. 
```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-irf
#| tbl-cap: Statistics Summary 

df <- na.omit(df.log)
s <- matrix(nrow = 7,ncol = 5)
colnames(s) <- c("N","Mean","St.Dev.", "Min","Max")
rownames(s) <- colnames(df.log)

for (i in 1:7){
    s[i,1] <- length(na.omit(df[,i])) 
    s[i,2] <- mean(na.omit(df[,i])) 
    s[i,3] <- sd(na.omit(df[,i])) 
    s[i,4] <- min(na.omit(df[,i])) 
    s[i,5] <- max(na.omit(df[,i])) 
  }

knitr::kable(s, digits = 3,index=TRUE)
```


```{r data visualization}
#| echo: false
#| message: false
#| warning: false
#| label: fig-line-plot
#| fig-cap: "Time Series Plots (green lines are log transformation)"

dates <- as.Date(rownames(df),format = "%Y-%m-%d")
col <- c("#009999","#009999", "#009999", "#009999", "#9933CC", "#9933CC", "#009999")
names <- c("Real GDP per-capita","Government Spending","Government Revenue","Pubic Debt","Inflation","Cash Rate Target","Stock Price")

par(mfrow=c(4,2), mar=c(2,2,2,2))
for (i in 1:ncol(df)){
  plot(dates, y = df[,i], type = "l", 
       main = paste(names [i]), ylab = "", xlab = "",
       col = col[i], lwd = 2.5,
       ylim = c(min(na.omit(df[,i])),max(na.omit(df[,i]))))
}

```


# Preliminary Results 

ACF Plots as in @fig-acf-plot show non-zero autocorrelations for at least 15 lags of variables except Inflation. PACF Plots as @fig-pacf-plot indicate significant and strong memory at the first lag of variables except Inflation, of, for which the first lag is also statistically significant but not strong.  

```{r data properties: ACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-acf-plot
#| fig-cap: "ACF Plots"
par(mfrow=c(4,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(df))){
  acf(na.omit(df[,j]),main="")
  title(main = paste(names[j]), line = 1)
}
```

```{r data properties: PACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pacf-plot
#| fig-cap: "PACF Plots"
par(mfrow=c(4,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(df))){
  pacf(na.omit(df[,j]),main="")
  title(main = paste(names[j]), line = 1)
}
```

Augmented Dickey-Fuller Test can be performed to test for stationarity under the null hypothesis of unit-root non-stationary, the results are shown as @tbl-adf, from which we do not reject the null and conclude that all variables are unit-root non-stationary.  

```{r test for stationarity}
#| echo: true 
#| message: false
#| warning: false
#| label: tbl-adf
#| tbl-cap: ADF test results 

library(tseries)
adf <- as.data.frame(matrix(nrow=7,ncol=3,NA))
rownames(adf) <- colnames(df)
colnames(adf) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in 1: ncol(df)){
  adf_tmp                 <- adf.test(df[,i])
  adf[i,"Dickey-Fuller"]  <-  round(as.numeric(adf_tmp[1]),3)
  adf[i,"Lag order"]      <-  as.numeric(adf_tmp[2])
  adf[i,"p-value"]        <-  round(as.numeric(adf_tmp[4]),3)
}

knitr::kable(adf, index=TRUE)
```

Take the first difference and run the Augmented Dickey-Fuller Test again, it is sensible to conclude that variables except public debt and interest rate are integrated of order 1 as in @tbl-adf-diff. Public debt is integrated with order 2, since its second-order difference is stationary as @tbl-adf-diff2.  

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-adf-diff
#| tbl-cap: First difference ADF test results 
adf.diff <- as.data.frame(matrix(nrow=7,ncol=3,NA))
rownames(adf.diff) <- colnames(df)
colnames(adf.diff) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in 1: ncol(df)){
  tmp <- adf.test(diff(df[,i]))
  adf.diff[i,"Dickey-Fuller"] <-  round(as.numeric(tmp[1]),3)
  adf.diff[i,"Lag order"]<-  as.numeric(tmp[2])
  adf.diff[i,"p-value"] <-  round(as.numeric(tmp[4]),3)
}

knitr::kable(adf.diff, index=TRUE)

```


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-adf-diff2
#| tbl-cap: Second difference ADF test results 
adf.diff2 <- as.data.frame(matrix(nrow=1,ncol=3,NA))
rownames(adf.diff2) <- c("Debt")
colnames(adf.diff2) <- c("Dickey-Fuller","Lag order", "p-value")
adf.diff2[,"Dickey-Fuller"] <-  round(as.numeric(adf.test(diff(diff(df$Debt)))[1]),3)
adf.diff2[,"Lag order"]<-  as.numeric(adf.test(diff(diff(df$Debt)))[2])
adf.diff2[,"p-value"] <-  round(as.numeric(adf.test(diff(diff(df$Debt)))[4]),3)
knitr::kable(adf.diff2, index=TRUE)
```



# Methodology

Bayesian Structural vector autoregression (SVAR) models to capture the dynamic and contemporaneous relationships between variables. The benchmark model is defined as: 

\begin{align}
B_0Y_t = b_0 + \sum_{i=1}^{p}B_{i}Y_{t-i}+u_t 
\end{align}
\begin{align}
u_t|Y_{t-1}\sim iid(0_N,I_N)
\end{align}

$Y_t$ is $N\times1$ matrix of endogenous variables,N represents the number of endogeneous variables and p is the number of lags. $B_0$ is $N\times N$ matrix capturing contemporaneous relationships between variables, and $u_t$ is $N\times1$ vector conditionally on $Y_{t-1}$ orthogonal structural shocks. 

In particular, in this model, $Y_t$ contains seven variables ordered as : (1) real per-capita GDP ($G_t$), (2) real per-capita government expenditure ($E_t$), (3) real per-capita government revenue ($R_t$), (4) real per-capita public debt ($D_t$), (5) inflation ($i_t$), (6) cash rate target ($r_t$), (7) real stock price ($S_t$).

\begin{align}
Y_t=\begin{pmatrix} 
\\ G_t
\\ E_t
\\ R_t 
\\ D_t
\\ i_t
\\ r_t
\\ S_t
\end{pmatrix}
\end{align}

The reduced form representation is: 
\begin{align}
Y_t = \mu_0 + \sum_{i=1}^{p}A_{i}Y_{t-i}+\epsilon_t
\end{align}
\begin{align}
\epsilon_t|Y_{t-1}\sim iid(0_N,\Sigma)
\end{align}

The covariance matrix of $\epsilon_t$ can be written as:
\begin{align}
\Sigma=B_0^{-1}{B_0^{-1}}^{'}
\end{align}

# Estimation 

```{r}
# create a numeric df, which would benefit computing 
df.num <- cbind(
  as.numeric(df$GDP),
  as.numeric(df$Spending),
  as.numeric(df$Revenue),
  as.numeric(df$Debt),
  as.numeric(df$Inflation),
  as.numeric(df$Interest),
  as.numeric(df$Stock)
)
colnames(df.num) <- colnames(df)
```



## Basic Model 



```{r}
#| echo: true
GetPrior.parameters <- function (N,p,kappa.1=1^2,kappa.2=100,Sigma.hat) {
  
  A.prior = matrix(0,(1+N*p),N)
  A.prior[2:(N+1),] = diag(N) 
  V.prior = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior = diag(diag(Sigma.hat))
  nu.prior = N+1
  
  return (list(A.prior = A.prior,
               V.prior = V.prior,
               S.prior = S.prior,
               nu.prior = nu.prior))
}
```

```{r}
#| echo: true
GetPoesterior.parameters <- function (X,Y,A.prior,nu.prior,S.prior,V.prior) {
  
  V.bar.inv <- t(X)%*%X + diag(1/diag(V.prior))
  V.bar <- solve(V.bar.inv)
  A.bar <- V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar <- nrow(Y) + nu.prior
  S.bar <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  
  return (list(V.bar = V.bar,
               A.bar = A.bar,
               nu.bar = nu.bar,
               S.bar = S.bar))
}
```

```{r draw poesterior parameters}
#| echo: true
DrawPosterior <- function (N,S,p,nu.bar,S.bar,A.bar,V.bar){
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=solve(S.bar))  
  Sigma.posterior   = apply(Sigma.posterior,3,solve)            
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))   
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) 
  B.posterior       = array(NA,c(N,N,S))
  L                 = t(chol(V.bar)) 
  B1.tilde.s        = array(NA,c(N,(1+N*p),S))
  
  for (s in 1:S){
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B.posterior[,,s]= t(cholSigma.s) 
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s 
    B1.tilde.s[,,s] = B.posterior[,,s]%*%t(A.posterior[,,s])
  }
  
  # A.draws <- round(apply(A.posterior,1:2,mean),4)  # (Np+1) by N 
  # B.draws <- round(apply(B.posterior,1:2,mean),4)  # N by N
  # B1.draws <- round(apply(B.posterior,1:2,mean),4) # N by (1+Np)
  # Sigma.draws <-round(apply(Sigma.posterior,1:2,mean),4)
  
  return(list(A.posterior = A.posterior,
              B.posterior = B.posterior,
              B1.tilde.s = B1.tilde.s,
              Sigma.posterior = Sigma.posterior)
         )
}
```

```{r}
#| echo: true
ImposeSignRestriction <- function (restrictions, N,S,p,A.posterior,Sigma.posterior,B.posterior,B1.tilde.s){
  
  R1 <- diag(restrictions)
  B0.draws      = array(NA,c(N,N,S))
  B1.draws      = array(NA,c(N,(1+N*p),S))
  i.vec = c()

  for (s in 1:S){
    A             = A.posterior[,,s]
    Sigma         = Sigma.posterior[,,s]
    B0.tilde      = B.posterior[,,s]
    B1.tilde      = B1.tilde.s[,,s]
    
    i=1
    sign.restrictions.do.not.hold = TRUE 
    while (sign.restrictions.do.not.hold){
    X           = matrix(rnorm(N*N),N,N)          # draw iid normal X 
    QR          = qr(X, tol = 1e-10)
    Q           = qr.Q(QR,complete=TRUE)
    R           = qr.R(QR,complete=TRUE)
    Q           = t(Q %*% diag(sign(diag(R))))
    B0          = Q%*%B0.tilde                    # 7 by 7 
    B1          = Q%*%B1.tilde                    # 7 by 7 
    B0.inv      = solve(B0)      
    check       = prod(R1 %*% B0.inv %*% diag(N)[,2] > 0)
    if (check==1){sign.restrictions.do.not.hold=FALSE}
    i=i+1
    }
    i.vec = c(i.vec,i)
    B0.draws[,,s] = B0
    B1.draws[,,s] = B1
  }
  return (list(B0.draws = B0.draws,
               B1.draws = B1.draws))
}

```


### Not using Functions 

```{r prior distribution}
# # Minnesota prior
# # dimension of A.prior should match (1+Np)*N -> 43 * 12 in this case
# kappa.1     = 1^2
# kappa.2     = 100
# kappa.3     = 1
# A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
#   # first row of A.prior is 0s
#   # second to (N+1)th row is kappa.3*Identity(N)
#   # the rest rows are 0s
# A.prior[2:(N+1),] = kappa.3*diag(N)
# V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))) # dimension is (Np+1)*(Np+1)
# S.prior     = diag(diag(Sigma.hat))  # dimension is N*N
# nu.prior    = N+1                    # scalar
```

```{r normal-inverse Wishard posterior parameters}
# V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))  # diag(1/diag(M)) = M ^ (-1)
# V.bar       = solve(V.bar.inv)
# A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
# nu.bar      = nrow(Y) + nu.prior
# S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
# S.bar.inv   = solve(S.bar)
```

```{r posterior draw}
# Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
# Sigma.posterior   = apply(Sigma.posterior,3,solve)
# Sigma.posterior   = array(Sigma.posterior,c(N,N,S))   # reshape into S@N*N matrices
# 
# A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
# # draw 43*7*10000 normal random variables, and reshape
# B.posterior       = array(NA,c(N,N,S))
# L                 = t(chol(V.bar)) # Compute the transpose of Choleski factorization of matrix
# B1.tilde.s        = array(NA,c(N,(1+N*p),S)) # the dimension is N by (1+N*p)
# 
# for (s in 1:S){
#   cholSigma.s     = chol(Sigma.posterior[,,s])
#   B.posterior[,,s]= t(cholSigma.s) ### this is B0.tilde.s, 7 by 7
#   A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s # this is 29 by 7
#   B1.tilde.s[,,s] = B.posterior[,,s]%*%t(A.posterior[,,s])
# }
# 
# A.draws <- round(apply(A.posterior,1:2,mean),4)  # (Np+1) by N
# B.draws <- round(apply(B.posterior,1:2,mean),4)  # N by N
# B1.draws <- round(apply(B.posterior,1:2,mean),4) # N by (1+Np)
# Sigma.draws <-round(apply(Sigma.posterior,1:2,mean),4)
```

```{r}
# define government spending shocks 
# restrictions = c(-1,-1,-1, 1, 1,-1,-1)    
# 
# R1 <- diag(restrictions)
# B0.draws      = array(NA,c(N,N,S))
# B1.draws      = array(NA,c(N,(1+N*p),S))
# i.vec = c()
# 
# pb = txtProgressBar(min = 0, max = S, initial = 0, style = 1) 
# for (s in 1:S){
#   setTxtProgressBar(pb,s)
#   
#   A             = A.posterior[,,s]
#   Sigma         = Sigma.posterior[,,s]
#   B0.tilde     = B.posterior[,,s]
#   B1.tilde      = B1.tilde.s[,,s]
#   i=1
#   sign.restrictions.do.not.hold = TRUE 
#   
#   while (sign.restrictions.do.not.hold){
#   X           = matrix(rnorm(N*N),N,N) # draw iid normal X 
#   QR          = qr(X, tol = 1e-10)
#   Q           = qr.Q(QR,complete=TRUE)
#   R           = qr.R(QR,complete=TRUE)
#   Q           = t(Q %*% diag(sign(diag(R))))
#   B0          = Q%*%B0.tilde # N by N 
#   B1          = Q%*%B1.tilde # 7 by 7 
#   B0.inv      = solve(B0)
#   check       = prod(R1 %*% B0.inv %*% diag(N)[,2] > 0)
#   if (check==1){sign.restrictions.do.not.hold=FALSE}
#   i=i+1
#   }
#   
#   # print(i)
#   i.vec = c(i.vec,i)
#   B0.draws[,,s] = B0
#   B1.draws[,,s] = B1
# }
# close(pb)
```


## Extented Model 

```{r}
GetPrior.parameters.g <- function (N,p,Sigma.hat,nu.kappa,S.kappa,Extended) {
 
  if (Extended) {
    kappa.1 <- S.kappa / rchisq(1, df=nu.kappa) # IG2 draw 
    kappa.2 <- 10 * kappa.1
  } else {
    kappa.1 <- 1 
    kappa.2 <- 100 
  }
  
  A.prior = matrix(0,(1+N*p),N)
  A.prior[2:(N+1),] = diag(N) 
  V.prior = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior = diag(diag(Sigma.hat))
  nu.prior = N+1
  
  return (list(A.prior=A.prior,
               V.prior = V.prior,
               S.prior = S.prior,
               nu.prior = nu.prior,
               kappa = kappa.1))
}
```

```{r}
# # This functions should work
# GetPoesterior.parameters.e <- function (X,Y,A.prior,nu.prior,S.prior,V.prior,kappa) {
#   
#   V.bar.inv <- t(X)%*%X + solve(kappa*V.prior)
#   V.bar <- solve(V.bar.inv)
#   A.bar <- V.bar%*%(t(X)%*%Y + diag(1/diag(kappa*V.prior))%*%A.prior)
#   nu.bar <- nrow(Y) + nu.prior
#   S.bar <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(kappa*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
#   
#   return (list(V.bar = V.bar,
#                A.bar = A.bar,
#                nu.bar = nu.bar,
#                S.bar = S.bar))
# }
```

Construction of Gibbs Sampler Algorithm: 

Initialize $\kappa$ at $\kappa^{0}$. 

At each iteration s: 

1. Draw $(A,\Sigma)^{s} \sim p(A,\Sigma| X,Y,\kappa^{s-1})$
2. Draw $\kappa^{s} \sim p(\kappa|Y,X,A,\Sigma)$

Repeat 1 and 2 for $(S_1 + S_2)$ times.

Discard the first $S_1$. 

```{r}
DrawPosterior.e <- function (N,p,nu.prior,S.bar,A.bar,S1,S2){
  
  kappa <- c()
  A.posterior <- array(NA,c((1+N*P),N,(S1+S2)))
  s <- array(c(N,N,(S1+S2))) 
  B.posterior       = array(NA,c(N,N,(S1+S2)))
  B1.tilde.s        = array(NA,c(N,(1+N*p),(S1+S2)))
  
  # Initialize kappa.0 
  kappa[1] <- 1 
  
  # Not affected by kappa value 
  nu.bar <- nu.prior + nrow(Y)
  nu.kappa.bar <- nu.kappa + N*nrow(Y) # N*K 
 
  for (s in 1:(S1+S2)){
    # STEP 1: draw (A Sigma).s from MNIW(A.bar,V.bar,S.bar,nu.bar) thich is conditional on kappa.(s-1)
              # Note here I made list of kappa, when kappa[1] = initial value (kappa.0) 
              # therefore, kappa[s] = kappa.(s-1)
    V.bar.inv <- t(X)%*%X + solve(kappa[s]*V.prior)
    V.bar     <- solve(V.bar.inv)
     
    
    A.bar     <- V.bar%*%(t(X)%*%Y + diag(1/diag(kappa[s]*V.prior))%*%A.prior)
    S.bar     <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(kappa[s]*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
              
              # Now posterior parameters are computed using kappa values, then we can draw (A Sigma)
    Sigma.posterior[,,s]   = solve(rWishart(1, df=nu.bar, Sigma=solve(S.bar))) 
    cholSigma.s            = chol(Sigma.posterior[,,s])
    A.posterior[,,s]       = matrix(rmvnorm(1,mean=as.vector(A.bar),sigma=Sigma.posterior[,,s]%*%V.bar),ncol=N)               # from lec10 - page 12 
    
    L               <- t(chol(V.bar))
    B1.tilde.s[,,s] <- B.posterior[,,s]%*%t(A.posterior[,,s])
    
    # STEP 2: draw kappa.s from IG2(S.bar, nu.bar)
              # Note kappa.s = kappa[s+1]
    S.kappa.bar <- S.kappa + trace(solve(Sigma.posterior[,,s]) %*% t(A.posterior[,,s]-A.prior)%*% solve(V.prior) %*% (A.posterior[,,s]-A.prior))
    kappa[s+1]  <- S.kappa.bar / rchisq(1, df=nu.kappa.bar)
  }                  
}
```

## Create Artificial Data   



```{r generate bi-variate Gaussian random walk process}
set.seed(1)
RW1 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)
plot.ts(RW1,main="Random Walk 1", col=4)
plot.ts(diff(RW1),main="First difference of Random Walk 1", col=4)

RW2 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)
plot.ts(RW2,main="Random Walk 2", col=4)
plot.ts(diff(RW2),main="First difference of Random Walk 2", col=4)

RW  <- cbind(RW1,RW2)
```

```{r artificial data setting}
Y       = RW[2:nrow(RW),]
X       = matrix(1,nrow(Y),1)
X       = cbind(X,RW[2: nrow(RW)-1,])

N       = 2      # number of variables
p       = 1      # number of lags
S       = 10000  # sample size
h       = 8      # number of periods ahead of forecasting
K       = p + 1

A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y                
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)   

restrictions = c(-1,1)
```

```{r set-up, get design matrices and MLE}
# #| echo: true
# #| message: false
# #| warning: false
# N       = 7      # number of variables 
# p       = 4      # number of lags 
# S       = 10000  # sample size 
# h       = 8      # number of periods ahead of forecasting 
# K       = p + 1 
# 
# df.len  = nrow(df.num)
# Y       = ts(df.num[K:df.len,], start=c(1991,3), frequency=4)
# X       = matrix(1,nrow(Y),1)
# 
# # X is the design matrix, containing 1s and lags of Y 
# for (i in 1:p){
#   X     = cbind(X,df.num[K:df.len-i,])
# }
# 
# A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y                # (X'X)^(-1)X'Y
# Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)    # lecture 7 slides 29 

# restrictions = c(-1,-1,-1, 1, 1,-1,-1) 
```


```{r generating prior parameters}
prior.parameters = GetPrior.parameters(N,p,kappa.1=1^2,kappa.2=100,Sigma.hat)
A.prior <- prior.parameters$A.prior
V.prior <- prior.parameters$V.prior
S.prior <- prior.parameters$S.prior
nu.prior <- prior.parameters$nu.prior
```

```{r generating normal-inverse Wishard posterior parameters}
poesterior.parameters.iw <- GetPoesterior.parameters(X,Y,A.prior,nu.prior,S.prior,V.prior)

A.bar <- poesterior.parameters.iw$A.bar
V.bar <- poesterior.parameters.iw$V.bar
S.bar <- poesterior.parameters.iw$S.bar
nu.bar <- poesterior.parameters.iw$nu.bar
```

```{r}
posterior.draws <- DrawPosterior(N,S,p,nu.bar,S.bar,A.bar,V.bar)

A.posterior <- posterior.draws$A.posterior
B.posterior <- posterior.draws$B.posterior
B1.tilde.s <- posterior.draws$B1.tilde.s
Sigma.posterior <- posterior.draws$Sigma.posterior
```

```{r apply sign restriction}
B.draws.restricted <- ImposeSignRestriction(restrictions, N,S,p,A.posterior,Sigma.posterior,B.posterior,B1.tilde.s)

B0.draws <- B.draws.restricted$B0.draws
B1.draws <- B.draws.restricted$B1.draws
```

```{r check do they produce true parameters using simulated data}
A.check <- array(NA,c(N+1,N,S))
S.check <- array(NA,c(N,N,S))
for (s in 1:S){
  # convert Bo into Sigma 
  S.check[,,s] <- B0.draws[,,s] %*% t(B0.draws[,,s])
  A.check[,,s] <- t(B1.draws[,,s]) %*% B0.draws[,,s]
}

round(apply(A.check,1:2,mean),4)
round(apply(S.check,1:2,mean),4)
```


# Code reservation 
```{r}
# kappa.1     = 1^2
# kappa.2     = 100
# kappa.3     = 1
# A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
# A.prior[2:(N+1),] = kappa.3*diag(N)
# V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))) 
# S.prior     = diag(diag(Sigma.hat)) 
# nu.prior    = N+1                   
```

```{r prior distribution2x}
# # Minnesota prior
# # dimension of A.prior should match (1+Np)*N -> 43 * 12 in this case
# kappa.1     = 1^2
# kappa.2     = 100
# kappa.3     = 1
# A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
#   # first row of A.prior is 0s
#   # second to (N+1)th row is kappa.3*Identity(N)
#   # the rest rows are 0s
# A.prior[2:(N+1),] = kappa.3*diag(N)
# V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))) # dimension is (Np+1)*(Np+1)
# S.prior     = diag(diag(Sigma.hat))  # dimension is N*N
# nu.prior    = N+1                    # scalar
```

```{r normal-inverse Wishard posterior parameters2}
# V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))  # diag(1/diag(M)) = M ^ (-1)
# V.bar       = solve(V.bar.inv)
# A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
# nu.bar      = nrow(Y) + nu.prior
# S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
# S.bar.inv   = solve(S.bar)
```

```{r posterior draw2}
# Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
# Sigma.posterior   = apply(Sigma.posterior,3,solve)
# Sigma.posterior   = array(Sigma.posterior,c(N,N,S))   # reshape into S@N*N matrices
# 
# A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
# # draw 43*7*10000 normal random variables, and reshape
# B.posterior       = array(NA,c(N,N,S))
# L                 = t(chol(V.bar)) # Compute the transpose of Choleski factorization of matrix
# B1.tilde.s        = array(NA,c(N,(1+N*p),S)) # the dimension is N by (1+N*p)
# 
# for (s in 1:S){
#   cholSigma.s     = chol(Sigma.posterior[,,s])
#   B.posterior[,,s]= t(cholSigma.s) ### this is B0.tilde.s, 7 by 7
#   A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s # this is 29 by 7
#   B1.tilde.s[,,s] = B.posterior[,,s]%*%t(A.posterior[,,s])
# }
# 
# A.draws <- round(apply(A.posterior,1:2,mean),4)  # (Np+1) by N
# B.draws <- round(apply(B.posterior,1:2,mean),4)  # N by N
# B1.draws <- round(apply(B.posterior,1:2,mean),4) # N by (1+Np)
# Sigma.draws <-round(apply(Sigma.posterior,1:2,mean),4)
```

```{r}
# restrictions = c(-1,1) 
# R1 <- diag(restrictions)
# B0.draws      = array(NA,c(N,N,S))
# B1.draws      = array(NA,c(N,(1+N*p),S))
# i.vec = c()
# 
# pb = txtProgressBar(min = 0, max = S, initial = 0, style = 1) 
# for (s in 1:S){
#   setTxtProgressBar(pb,s)
#   
#   A             = A.posterior[,,s]
#   Sigma         = Sigma.posterior[,,s]
#   B0.tilde     = B.posterior[,,s]
#   B1.tilde      = B1.tilde.s[,,s]
#   i=1
#   sign.restrictions.do.not.hold = TRUE 
#   
#   while (sign.restrictions.do.not.hold){
#   X           = matrix(rnorm(N*N),N,N) # draw iid normal X 
#   QR          = qr(X, tol = 1e-10)
#   Q           = qr.Q(QR,complete=TRUE)
#   R           = qr.R(QR,complete=TRUE)
#   Q           = t(Q %*% diag(sign(diag(R))))
#   B0          = Q%*%B0.tilde # N by N 
#   B1          = Q%*%B1.tilde # 7 by 7 
#   B0.inv      = solve(B0)
#   check       = prod(R1 %*% B0.inv %*% diag(N)[,2] > 0)
#   if (check==1){sign.restrictions.do.not.hold=FALSE}
#   i=i+1
#   }
#   
#   # print(i)
#   i.vec = c(i.vec,i)
#   B0.draws[,,s] = B0
#   B1.draws[,,s] = B1
# }
# close(pb)
```



# References {.unnumbered}

Afonso, A., R. M. Sousa (2011): What are the effects of Fiscal policy on asset markets? *Economic Modelling*, 28, 1871-1890.

Blanchard O, Perotti R. (2002). An empirical characterization of the dynamic effects of changes in government spending and taxes on output. *Quarterly Journal of Economics*. 117(4): 1329â€“1368.

Chatziantoniou, I., D. Duffy, G. Filis (2013): Stock market response to monetary and Fiscal policy shocks: Multi-country evidence, *Economic Modelling*, 30, 754-769.

Mountford, A., H. Uhlig (2009): What are the effects of Fiscal policy shocks? *Journal of Applied Econometrics*, 24, 960-992.

Mumtaz, H., Theodoridis, K. (2020). Fiscal policy shocks and stock prices in the United States, *European Economic Review*, Volume 129

Romer, C.D. and Romer, D.H. (2007) The Macroeconomic Effects of Tax Changes: Estimates Based on a New Measure of Fiscal Shocks. *NBER Working Paper No. 13264, National Bureau of Economic Research, Cambridge.*