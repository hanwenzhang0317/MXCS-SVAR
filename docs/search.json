[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fiscal Policy Shocks, Aggregate Economy and Stock Prices: Evidence from the Australian Economy",
    "section": "",
    "text": "Abstract. This is a reserarch proposal, the project is about measuring fiscal policy effects using Bayesian Structural Vector Autoregression (SVAR) in Australian Economy. Impulse responses of stock prices and macroeconomic aggregates will be investigated.\nKeywords. fiscal policy shock, SVAR, tax shocks, stock price, impulse response function"
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "Fiscal Policy Shocks, Aggregate Economy and Stock Prices: Evidence from the Australian Economy",
    "section": "Basic Model",
    "text": "Basic Model\nRewrite the reduced form equation in matrix: \\[\\begin{gather}\nY = XA + E \\\\\n\\\\ E|X \\sim MN_{T \\times N}(0_{T \\times N},\\Sigma,I_T)\n\\end{gather}\\] The Likelihood function would be: \\[\\begin{gather}\nL(A,\\Sigma|Y,X) \\propto det(\\Sigma)^{-\\frac{T}{2}} exp \\left\\{-\\frac{1}{2} tr \\left[ \\Sigma^{-1}(Y-XA)'(Y-XA) \\right] \\right\\} \\\\\n\\\\ \\propto det(\\Sigma)^{-\\frac{T}{2}} exp \\left\\{-\\frac{1}{2} tr \\left[ \\Sigma^{-1}(A-\\hat{A})'X'X(A-\\hat{A}) \\right] \\right\\} exp \\left\\{-\\frac{1}{2} tr \\left[\\Sigma^{-1}(Y-X \\hat{A})'(Y-X \\hat{A}) \\right] \\right\\} \\\\\n\\end{gather}\\] From Maximum Likelihood Estimation, \\[\\begin{gather}\n\\hat{A} = (X'X)^{-1}X'Y \\\\\n\\\\ \\hat{\\Sigma} = \\frac{1}{T} (Y-X \\hat{A})'(Y-X \\hat{A})\n\\end{gather}\\]\nIn the basic model, we have prior as: \\[\\begin{gather}\np(A,\\Sigma) = p(A|\\Sigma) p(\\Sigma) \\\\\n\\\\ A|\\Sigma \\sim MN_{K \\times N} (\\underline{A}, \\Sigma , \\underline{V}) \\\\\n\\\\ \\Sigma \\sim IW_{N}(\\underline{S},\\underline{\\nu})\n\\end{gather}\\]\nParameters are as follows: \\[\\begin{gather}\n\\underline{A} = [0_{N \\times 1}, I_N, 0_{N \\times (p-1)N}]' \\\\\n\\\\ Var[vec(A)] = \\Sigma \\otimes  \\underline{V} \\\\\n\\\\ \\underline{V} = diag([\\kappa_2 \\quad \\kappa_1 (p^{-2} \\otimes I_N)]) \\\\\n\\\\ p = [1,2,...p]\n\\end{gather}\\]\nThe full conditional posterior is: \\[\\begin{gather}\np(A,\\Sigma|Y,X) = p(A|Y,X,\\Sigma)p(\\Sigma|Y,X) \\\\\n\\\\ p(A|Y,X,\\Sigma) = MN_{K \\times N}(\\bar{A}, \\Sigma, \\bar{V}) \\\\\n\\\\ p(\\Sigma | Y, X) = IW_N(\\bar{S},\\bar{\\nu})\n\\end{gather}\\]\nDerive the full conditional posterior: \\[\\begin{gather}\nP(A,\\Sigma|Y,X) \\propto L(A,\\Sigma|Y,X)p(A,\\Sigma) \\\\\n\\\\ \\propto L(A,\\Sigma|Y,X)p(A|\\Sigma)p(\\Sigma) \\\\\n\\\\ det(\\Sigma)^{-\\frac{T}{2}} \\times exp \\left\\{-\\frac{1}{2} tr \\left[ \\Sigma^{-1}(A-\\hat{A})' X'X (A-\\hat{A})\\right] \\right\\} \\\\\n\\\\ \\times exp\\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1}(Y-X\\hat{A})'(Y-X\\hat{A}) \\right] \\right\\} \\\\\n\\\\ \\times det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n\\\\ \\times exp\\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1}(A-\\underline{A})'\\underline{V}^{-1}(A-\\underline{A}) \\right] \\right\\} \\\\\n\\\\ \\times exp \\left\\{ -\\frac{1}{2} tr \\left[ \\Sigma^{-1} \\underline{S} \\right] \\right\\}\n\\end{gather}\\]\n\\[\\begin{gather}\np(A,\\Sigma|Y,X) \\propto det{(\\Sigma)}^{-\\frac{T+N+K+ \\underline{\\nu}\n+1}{2}} \\times exp \\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1} \\left[(A-\\hat{A})^{'}X'X(A-\\hat{A})+(A-\\underline{A})^{'} \\underline{V}^{-1}(A-\\underline{A}) + (Y-X\\hat{A})^{'}(Y-X\\hat{A})+\\underline{S}  \\right]\\right] \\right\\}\\\\\n\\\\ \\propto det{(\\Sigma)}^{-\\frac{T+N+K+ \\underline{\\nu}\n+1}{2}} \\times exp\\left\\{ -\\frac{1}{2} tr \\left[ \\Sigma^{-1} \\left[ (A-\\bar{A})^{'} \\bar{V}^{-1} (A-\\bar{A})+\\underline{S} +Y^{'}Y + \\underline{A}^{'} \\underline{V}^{-1}\\underline{A} -\\bar{A}^{'} \\bar{V}^{-1}\\bar{A}\\right]\\right]\\right\\}\n\\end{gather}\\]\nwhere we have posterior distribution parameters:\n\\[\\begin{gather}\n\\bar{V} = (X^{'}X+ \\underline{V}^{-1})^{-1} \\\\\n\\\\ \\bar{A} = \\bar{V}(X^{'}Y+\\underline{V}^{-1} \\underline{A}) \\\\\n\\\\ \\bar{\\nu} = T + \\underline{\\nu} \\\\\n\\\\ \\bar{S} = \\underline{S} + Y^{'}Y +  A^{'}\\underline{V}^{-1}\\underline{A} - \\bar{A}^{'}\\bar{V}^{-1}\\bar{A}\n\\end{gather}\\]\nFrom above derivation, we can compute functions to calculate parameters:\n\n\n\n\nGetPoesterior.parameters <- function (X,Y,A.prior,nu.prior,S.prior,V.prior) {\n  \n  V.bar.inv <- t(X)%*%X + diag(1/diag(V.prior))\n  V.bar <- solve(V.bar.inv)\n  A.bar <- V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n  nu.bar <- nrow(Y) + nu.prior\n  S.bar <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n  \n  return (list(V.bar = V.bar,\n               A.bar = A.bar,\n               nu.bar = nu.bar,\n               S.bar = S.bar))\n}\n\nThen posterior distributions of \\(A\\) and \\(\\Sigma\\) can be drawn:\n\nDrawPosterior <- function (N,S,p,nu.bar,S.bar,A.bar,V.bar){\n  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=solve(S.bar))  \n  Sigma.posterior   = apply(Sigma.posterior,3,solve)            \n  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))   \n  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) \n  B.posterior       = array(NA,c(N,N,S))\n  L                 = t(chol(V.bar)) \n  B1.tilde.s        = array(NA,c(N,(1+N*p),S))\n  \n  for (s in 1:S){\n    cholSigma.s     = chol(Sigma.posterior[,,s])\n    B.posterior[,,s]= t(cholSigma.s) \n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s \n    B1.tilde.s[,,s] = B.posterior[,,s]%*%t(A.posterior[,,s])\n  }\n\n  return(list(A.posterior = A.posterior,\n              B.posterior = B.posterior,\n              B1.tilde.s = B1.tilde.s,\n              Sigma.posterior = Sigma.posterior)\n         )\n}\n\nAfter the posterior draws, we apply sign restrictions to identify the model. For any set of sign restrictions, given a parameter point \\(B_{+},B_{0}\\) that satisfies such restrictions, there always exists an orthogonal matrix Q, arbitrarily close to an identity such that \\(QB_{+},QB_{0}\\) satisfy the sign restrictions.\nThe Algorithm from Fry, Pagan (2011) is implemented to transfer SF parameters \\((\\tilde{B_+},\\tilde{B_0})\\) to parameters \\((B_{+},B_{0})\\) such the restrictions of interest holds.\n\nImposeSignRestriction <- function (restrictions,N,S,p,A.posterior,Sigma.posterior,B.posterior,B1.tilde.s){\n  \n  R1 <- diag(restrictions)\n  B0.draws      = array(NA,c(N,N,S))\n  B1.draws      = array(NA,c(N,(1+N*p),S))\n  i.vec = c()\n\n  for (s in 1:S){\n    A             = A.posterior[,,s]\n    Sigma         = Sigma.posterior[,,s]\n    B0.tilde      = B.posterior[,,s]\n    B1.tilde      = B1.tilde.s[,,s]\n    \n    i=1\n    sign.restrictions.do.not.hold = TRUE \n    while (sign.restrictions.do.not.hold){\n    X           = matrix(rnorm(N*N),N,N)          # draw iid normal X \n    QR          = qr(X, tol = 1e-10)\n    Q           = qr.Q(QR,complete=TRUE)\n    R           = qr.R(QR,complete=TRUE)\n    Q           = t(Q %*% diag(sign(diag(R))))\n    B0          = Q%*%B0.tilde                    # N by N \n    B1          = Q%*%B1.tilde                    # N by N\n    B0.inv      = solve(B0)      \n    check       = prod(R1 %*% B0.inv %*% diag(N)[,1] > 0)\n    if (check==1){sign.restrictions.do.not.hold=FALSE}\n    i=i+1\n    }\n    i.vec = c(i.vec,i)\n    B0.draws[,,s] = B0\n    B1.draws[,,s] = B1\n  }\n  return (list(B0.draws = B0.draws,\n               B1.draws = B1.draws))\n}"
  },
  {
    "objectID": "index.html#extented-model",
    "href": "index.html#extented-model",
    "title": "Fiscal Policy Shocks, Aggregate Economy and Stock Prices: Evidence from the Australian Economy",
    "section": "Extented Model",
    "text": "Extented Model\nIn the extended model,instead of setting \\(\\kappa_1\\) and \\(\\kappa_2\\) as fixed values, the overall shrinkage level for auto-regressive slopes \\(\\kappa_1\\) is assumed to follow an inverse gamma 2 distribution \\(IG2(\\underline{S}_{\\kappa},\\underline{\\nu}_{\\kappa})\\), and shrinkage of constant term \\(\\kappa_2\\) can be set as \\(100 \\times \\kappa_1\\).\nUnder this setting, \\[\\begin{gather}\np(\\kappa|A,\\Sigma,Y,X) \\\\\n\\\\ p(A,\\Sigma|X,Y,\\kappa)\n\\end{gather}\\]\nThe full conditional posterior of \\((A,\\Sigma)\\) is: \\[\\begin{gather}\np(A,\\Sigma|X,Y,\\kappa) \\propto L(A,\\Sigma|Y,X) \\times p(A|\\Sigma,\\kappa) \\times p(\\Sigma) \\\\\n\\\\ \\propto det(\\Sigma)^{-\\frac{K}{2}} exp \\left\\{-\\frac{1}{2}tr \\left[ \\Sigma^{-1}(Y-XA)^{'}(Y-XA)\\right] \\right\\} \\\\\n\\\\ \\times exp \\left\\{ -\\frac{1}{2} tr \\left[ \\Sigma^{-1}(A-\\underline{A})^{'}(\\kappa \\underline{V})^{-1}(A-\\underline{A}) \\right] \\right\\} \\\\\n\\\\ \\times det(\\Sigma)^{\\frac{\\underline{\\nu}+N+1}{2}} exp\\left\\{-\\frac{1}{2} tr \\left[\\Sigma^{-1}\\underline{S} \\right] \\right\\}\n\\end{gather}\\]\nWe recognize kernel of matrix-normal inverse Wharst distribution, with parameters as follows: \\[\\begin{gather}\n\\bar{V} = (X'X+(\\kappa \\underline{V}))^{-1} \\\\\n\\\\ \\bar{A} = \\bar{V}(X'Y+(\\kappa \\underline{V}^{-1}\\underline{A})) \\\\ \\\\ \\bar{S} = \\underline{S}+Y'Y+A'(\\kappa \\underline{V})^{-1} - \\bar{A}^{'} \\bar{V}^{-1}\\bar{A} \\\\\n\\\\ \\bar{\\nu} = T + \\underline{\\nu}\n\\end{gather}\\]\nThe full-conditional posterior of \\(\\kappa\\) is: \\[\\begin{gather}\np(\\kappa | A,\\Sigma, Y,X) \\propto L(Y|X,A,\\Sigma) \\times p(\\kappa) \\times p(A|\\Sigma,\\kappa) \\times p(\\Sigma) \\\\\n\\\\ \\propto p(\\kappa) \\times p(A|\\Sigma,\\kappa) \\\\\n\\\\ \\propto (\\kappa)^{-\\frac{\\underline{\\nu}_{\\kappa}+2}{2}}\nexp\\left\\{ -\\frac{1}{2} \\frac{\\underline{S}_{\\kappa}}{\\kappa} \\right\\}  \\times exp\\left\\{-\\frac{1}{2}tr\\left[\\Sigma^{-1}(A-\\underline{A})^{'} \\frac{1}{\\kappa}(\\underline{V})^{-1} (A-\\underline{A})\\right]\\right\\} \\times det(\\kappa \\underline{V})^{-\\frac{N}{2}} \\\\\n\\\\ \\propto (\\kappa)^{-\\frac{\\underline{\\nu}_{\\kappa}+2+NK}{2}} exp \\left\\{-\\frac{1}{2} \\frac{1}{\\kappa} \\left[\\underline{S}_{\\kappa} + tr \\left[\\Sigma^{-1}(A-\\underline{A})^{'}\\underline{V}^{-1}(A-\\underline{A}) \\right]\\right] \\right\\}\n\\end{gather}\\] which we recognise the kernel of inverse gamma 2 distribution with \\[\\begin{gather}\n\\bar{S}_{\\kappa} = \\underline{S}_{\\kappa}+tr \\left[ \\Sigma^{-1} (A-\\underline{A})^{'} \\underline{V}^{-1} (A-\\underline{A})\\right] \\\\\n\\\\ \\bar{\\nu}_{\\kappa} = \\underline{\\nu}_{\\kappa}+NK\n\\end{gather}\\]\nGibbs Sampler can be conducted to get the posterior draws of the extended model, for which we can,\nInitialize \\(\\kappa\\) at \\(\\kappa^{(0)}\\).\nAt each iteration s:\n\nDraw \\((A,\\Sigma)^{(s)} \\sim p(A,\\Sigma| X,Y,\\kappa^{(s-1)})\\)\nDraw \\(\\kappa^{(s)} \\sim p(\\kappa|Y,X,A,\\Sigma)\\)\n\nRepeat 1 and 2 for \\((S_1 + S_2)\\) times.\nDiscard the first \\(S_1\\).\n\nDrawPosterior.e <- function (Y,X,nu.prior,A.prior,V.prior,S.prior,nu.kappa,S.kappa,S1,S2){\n  \n  N = ncol(Y)\n  K = ncol(X)\n  T = nrow(Y)\n  kappa           <- c()\n  A.posterior     <- array(NA,c(K,N,(S1+S2)))\n  Sigma.posterior <- array(NA,c(N,N,(S1+S2))) \n  B.posterior     <- array(NA,c(N,N,(S1+S2)))\n  B1.tilde.s      <- array(NA,c(N,K,(S1+S2)))\n  \n  # Initialize kappa.0 \n  kappa[1] <- 1 \n  \n  nu.bar <- nu.prior + T \n  nu.kappa.bar <- nu.kappa + N*K # N*K \n \n  for (s in 1:(S1+S2)){\n    # STEP 1: draw (A Sigma).s from MNIW(A.bar,V.bar,S.bar,nu.bar)\n    V.bar.inv <- t(X)%*%X + solve(kappa[s]*V.prior)\n    V.bar     <- solve(V.bar.inv)\n    \n    A.bar     <- V.bar%*%(t(X)%*%Y + diag(1/diag(kappa[s]*V.prior))%*%A.prior)\n    S.bar     <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(kappa[s]*V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n              \n    draw.sigma.inv = solve(rWishart(1, df=nu.bar, Sigma=solve(S.bar))[,,1])\n    Sigma.posterior[,,s] = draw.sigma.inv\n    \n    cholSigma.s            = chol(Sigma.posterior[,,s])\n    A.posterior[,,s]       = matrix(MASS::mvrnorm(1,as.vector(A.bar),Sigma.posterior[,,s]%x%V.bar),ncol=N)      \n    \n    L                <- t(chol(V.bar))\n    B.posterior[,,s] <- t(chol(Sigma.posterior[,,s])) \n    B1.tilde.s[,,s]  <- B.posterior[,,s]%*%t(A.posterior[,,s])\n    \n    # STEP 2: draw kappa.s from IG2(S.bar, nu.bar)\n    S.kappa.bar <- S.kappa + sum(diag( draw.sigma.inv %*% t(A.posterior[,,s]-A.prior)%*% diag(1/diag(V.prior)) %*% (A.posterior[,,s]-A.prior)))\n    kappa[s+1]  <- S.kappa.bar / rchisq(1, df=nu.kappa.bar)\n  }   \n  \n  return(list(Sigma.posterior.e = Sigma.posterior[,,(S1+1):(S1+S2)],\n              A.posterior.e = A.posterior[,,(S1+1):(S1+S2)],\n              B1.tilde.s.e = B1.tilde.s[,,(S1+1):(S1+S2)],\n              B.posterior.e = B.posterior[,,(S1+1):(S1+S2)],\n              kappa.e = kappa[(S1+1):(S1+S2)]))\n}\n\nThe sign restrictions would be implied in the same way as the basic model."
  },
  {
    "objectID": "index.html#artificial-data-estimation",
    "href": "index.html#artificial-data-estimation",
    "title": "Fiscal Policy Shocks, Aggregate Economy and Stock Prices: Evidence from the Australian Economy",
    "section": "Artificial Data Estimation",
    "text": "Artificial Data Estimation\n1000 observations of bi-variate Gaussian random walk processes with the covariance matrix equal to the identity matrix of order 2 are generated as Figure 5 to test whether the modeling frameworks are able to estimate true parameters of the data generating process.\n\npar(mfrow=c(2,2))\nset.seed(2023)\nRW1 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)\nplot.ts(RW1,main=\"Random Walk 1\", col=4)\nplot.ts(diff(RW1),main=\"First difference of Random Walk 1\", col=4)\n\nRW2 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)\nplot.ts(RW2,main=\"Random Walk 2\", col=4)\nplot.ts(diff(RW2),main=\"First difference of Random Walk 2\", col=4)\n\nRW  <- cbind(RW1,RW2)\n\n\n\n\nFigure 5: Bi-variate Gaussian random walk\n\n\n\n\n\nY       = RW[2:nrow(RW),]\nX       = matrix(1,nrow(Y),1)\nX       = cbind(X,RW[2: nrow(RW)-1,])\n\nN       = ncol(Y)      # number of variables\np       = 1            # number of lags\nS       = 50000        # sample size\nK       = N*p + 1\n\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y                \nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)   \n\nrestrictions = c(-1,1)\n\n\nBasic Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5 and Table 6 show the matrix of \\(A\\) and \\(\\Sigma\\), suggesting the basic model estimation using artificial data of 1 lag and constant term is showing zero posterior mean of the autoregressive and covariance matrices are close to an identity matrix and the posterior mean of the constant term is close to zeros too.\n\n\n\n\nTable 5: Basic Model - A\n\n\nV1\nV2\n\n\n\n\n0.0252\n0.0522\n\n\n1.0162\n-0.0008\n\n\n-0.0060\n0.9402\n\n\n\n\n\n\n\n\n\n\nTable 6: Basic Model - Sigma\n\n\nV1\nV2\n\n\n\n\n0.9821\n-0.0002\n\n\n-0.0002\n0.9802\n\n\n\n\n\n\n\n\nExtended Model\nThen we can use the same data to check whether the extension model is working.\n\n\n\n\n\n\nFigure 6 shows the distribution of Gibbs Sampler draws of \\(\\kappa\\), which is right skewed and strictly positive, which make it appropriate to parameterize variance parameters.\n\n\n\n\n\nFigure 6: Gibbs Sampler draws of kappa\n\n\n\n\n\n\n\n\n\n\nTable 7 and Table 8 also shows desired results, suggesting the extented model is also working.\n\n\n\n\nTable 7: Extended Model - A\n\n\n0.0255\n0.0520\n\n\n1.0160\n-0.0007\n\n\n-0.0060\n0.9405\n\n\n\n\n\n\n\n\n\n\nTable 8: Extended Model - Sigma\n\n\n0.9825\n0.0000\n\n\n0.0000\n0.9801"
  }
]